{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvitohawari/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-TensorFlow/blob/main/Chapter_16_nlp_with_rnns_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ffb440",
      "metadata": {
        "id": "39ffb440"
      },
      "source": [
        "# Chapter 16 (NLP, RNNs, Attention) — Explanation & Implementation Notebook\n",
        "\n",
        "This notebook is written to accompany **Chapter 16: NLP with RNNs & Attention** and demonstrates the key implementations:\n",
        "- IMDb sentiment classification with **Embedding → (Bi)GRU → Dense**\n",
        "- **Masking** padded tokens\n",
        "- **Attention pooling** for sequence summarization\n",
        "- **Multi-Head Self-Attention** classifier\n",
        "- A small **Transformer encoder** block with positional embeddings\n",
        "\n",
        "> Designed to be GitHub-friendly: clean structure, runnable cells, and clear comments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8dbbdc",
      "metadata": {
        "id": "bc8dbbdc"
      },
      "source": [
        "## 0) Setup\n",
        "\n",
        "We will use TensorFlow/Keras and the IMDb dataset included in Keras.  \n",
        "The dataset is already tokenized into integer word IDs. We'll pad/truncate sequences so we can batch them.\n",
        "\n",
        "**Key idea:** Reviews are variable-length sequences → padding makes them same length, and **masking** ensures the model ignores `<pad>` tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6500c6db",
      "metadata": {
        "id": "6500c6db"
      },
      "outputs": [],
      "source": [
        "import os, random, numpy as np, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f719d503",
      "metadata": {
        "id": "f719d503"
      },
      "source": [
        "## 1) Load and prepare IMDb\n",
        "\n",
        "We limit the vocabulary (`max_features`) and pad sequences to a fixed length (`maxlen`).\n",
        "- `0` is reserved for padding.\n",
        "- Setting `mask_zero=True` in the Embedding layer will create and propagate a mask (`token_id != 0`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff68852",
      "metadata": {
        "id": "bff68852"
      },
      "outputs": [],
      "source": [
        "max_features = 10_000   # vocabulary size\n",
        "maxlen = 200           # sequence length after padding/truncation\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)  # pads with 0 by default\n",
        "x_test  = keras.utils.pad_sequences(x_test,  maxlen=maxlen)\n",
        "\n",
        "print(\"Train:\", x_train.shape, y_train.shape)\n",
        "print(\"Test :\", x_test.shape,  y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed2856a",
      "metadata": {
        "id": "5ed2856a"
      },
      "source": [
        "## 2) Training utility\n",
        "\n",
        "To keep this notebook lightweight, we train for a small number of epochs by default.\n",
        "If you want stronger accuracy, increase `EPOCHS`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b1fde4",
      "metadata": {
        "id": "e5b1fde4"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1          # change to 3–10 for better accuracy\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def compile_and_train(model, name, epochs=EPOCHS):\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Model:\", name)\n",
        "    model.summary()\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=epochs,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=2\n",
        "    )\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "    return history, test_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5e943a",
      "metadata": {
        "id": "4e5e943a"
      },
      "source": [
        "## 3) Baseline RNN model: Embedding → GRU → Dense\n",
        "\n",
        "**Concept mapping (Chapter 16):**\n",
        "- **Embedding** converts word IDs into dense vectors.\n",
        "- **GRU** reads the sequence and learns temporal dependencies.\n",
        "- Final **sigmoid** outputs probability of positive sentiment.\n",
        "\n",
        "We enable masking with `mask_zero=True` so the RNN ignores padding tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44b3e39",
      "metadata": {
        "id": "c44b3e39"
      },
      "outputs": [],
      "source": [
        "def build_gru_baseline():\n",
        "    inputs = keras.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    x = layers.Embedding(max_features, 128, mask_zero=True)(inputs)\n",
        "    x = layers.GRU(64)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs, name=\"gru_baseline\")\n",
        "\n",
        "gru_baseline = build_gru_baseline()\n",
        "_ = compile_and_train(gru_baseline, \"GRU baseline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5156524",
      "metadata": {
        "id": "e5156524"
      },
      "source": [
        "## 4) Bidirectional GRU: reading forward + backward\n",
        "\n",
        "**Concept mapping (Chapter 16):** Bidirectional RNNs can use both past and future context in the sequence.\n",
        "This often helps NLP classification tasks because the meaning of a word can depend on what comes after it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9809a0",
      "metadata": {
        "id": "8b9809a0"
      },
      "outputs": [],
      "source": [
        "def build_bigru():\n",
        "    inputs = keras.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    x = layers.Embedding(max_features, 128, mask_zero=True)(inputs)\n",
        "    x = layers.Bidirectional(layers.GRU(64))(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs, name=\"bigru\")\n",
        "\n",
        "bigru = build_bigru()\n",
        "_ = compile_and_train(bigru, \"Bidirectional GRU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41178764",
      "metadata": {
        "id": "41178764"
      },
      "source": [
        "## 5) Attention pooling: learn which time steps matter\n",
        "\n",
        "Instead of compressing the whole sequence using the final hidden state, we can learn **attention weights** over time steps:\n",
        "1. Compute a score for each time step.\n",
        "2. Softmax over time → weights sum to 1.\n",
        "3. Weighted sum of hidden states → a single vector representation.\n",
        "\n",
        "This is a simple \"attention for pooling\" mechanism for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2483d6",
      "metadata": {
        "id": "dd2483d6"
      },
      "outputs": [],
      "source": [
        "class AttentionPooling(layers.Layer):\n",
        "    \"\"\"Simple attention pooling over time steps for classification.\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.score_dense = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # inputs: (batch, time, features)\n",
        "        scores = self.score_dense(inputs)                 # (batch, time, 1)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask: (batch, time) boolean; convert to float and add -inf to padded positions\n",
        "            mask = tf.cast(mask, tf.float32)\n",
        "            scores += (1.0 - tf.expand_dims(mask, -1)) * (-1e9)\n",
        "\n",
        "        weights = tf.nn.softmax(scores, axis=1)           # (batch, time, 1)\n",
        "        return tf.reduce_sum(weights * inputs, axis=1)    # (batch, features)\n",
        "\n",
        "def build_bigru_attention_pool():\n",
        "    inputs = keras.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    x = layers.Embedding(max_features, 128, mask_zero=True)(inputs)\n",
        "    x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)\n",
        "    x = AttentionPooling()(x)  # uses the propagated mask from Embedding\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs, name=\"bigru_attention_pool\")\n",
        "\n",
        "bigru_attpool = build_bigru_attention_pool()\n",
        "_ = compile_and_train(bigru_attpool, \"BiGRU + AttentionPooling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d42d736",
      "metadata": {
        "id": "2d42d736"
      },
      "source": [
        "## 6) Multi-Head Self-Attention classifier\n",
        "\n",
        "**Self-attention** lets every token attend to every other token.\n",
        "To avoid attending to padding, we build an explicit `attention_mask`.\n",
        "\n",
        "Pipeline:\n",
        "- Embedding (no mask propagation needed here; we use attention_mask)\n",
        "- MultiHeadAttention (self-attention)\n",
        "- Residual + LayerNorm\n",
        "- GlobalAveragePooling\n",
        "- Dense classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7902331b",
      "metadata": {
        "id": "7902331b"
      },
      "outputs": [],
      "source": [
        "def build_mha_classifier(embed_dim=128, num_heads=4, ff_dim=128, dropout=0.1):\n",
        "    inputs = keras.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "\n",
        "    # Token embedding (we'll handle masking manually for attention)\n",
        "    x = layers.Embedding(max_features, embed_dim)(inputs)\n",
        "\n",
        "    # Build attention mask: True for real tokens, False for padding\n",
        "    # Keras MHA expects shape broadcastable to (batch, query_len, key_len)\n",
        "    padding_mask = tf.not_equal(inputs, 0)  # (batch, time)\n",
        "    attn_mask = tf.expand_dims(padding_mask, axis=1)      # (batch, 1, time)\n",
        "\n",
        "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(\n",
        "        x, x, attention_mask=attn_mask\n",
        "    )\n",
        "    x = layers.Add()([x, attn_out])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    # Small feed-forward block\n",
        "    ff = keras.Sequential([\n",
        "        layers.Dense(ff_dim, activation=\"relu\"),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(embed_dim),\n",
        "    ])\n",
        "    ff_out = ff(x)\n",
        "    x = layers.Add()([x, ff_out])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs, name=\"mha_classifier\")\n",
        "\n",
        "mha_model = build_mha_classifier()\n",
        "_ = compile_and_train(mha_model, \"Multi-Head Self-Attention classifier\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74ea17d",
      "metadata": {
        "id": "b74ea17d"
      },
      "source": [
        "## 7) Transformer encoder block (mini)\n",
        "\n",
        "**Concept mapping (Chapter 16):**\n",
        "- Attention alone has no notion of order → add **positional embeddings**\n",
        "- Transformer encoder = Multi-Head Attention + Feed-Forward + Residual + LayerNorm\n",
        "\n",
        "Here we implement:\n",
        "- Learned positional embeddings\n",
        "- One encoder block\n",
        "- Pool + classify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf1e1e5",
      "metadata": {
        "id": "7bf1e1e5"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def call(self, x):\n",
        "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)  # (time, embed_dim)\n",
        "        x = self.token_emb(x)                # (batch, time, embed_dim)\n",
        "        return x + positions                 # broadcast add\n",
        "\n",
        "def transformer_encoder(x, attn_mask, embed_dim=128, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(\n",
        "        x, x, attention_mask=attn_mask\n",
        "    )\n",
        "    attn_out = layers.Dropout(dropout)(attn_out)\n",
        "    x = layers.Add()([x, attn_out])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    ff = keras.Sequential([\n",
        "        layers.Dense(ff_dim, activation=\"relu\"),\n",
        "        layers.Dropout(dropout),\n",
        "        layers.Dense(embed_dim),\n",
        "    ])\n",
        "    ff_out = ff(x)\n",
        "    x = layers.Add()([x, ff_out])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    return x\n",
        "\n",
        "def build_transformer_classifier(embed_dim=128, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    inputs = keras.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    x = PositionalEmbedding(maxlen, max_features, embed_dim)(inputs)\n",
        "\n",
        "    padding_mask = tf.not_equal(inputs, 0)   # (batch, time)\n",
        "    attn_mask = tf.expand_dims(padding_mask, axis=1)  # (batch, 1, time)\n",
        "\n",
        "    x = transformer_encoder(x, attn_mask, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    return keras.Model(inputs, outputs, name=\"transformer_classifier\")\n",
        "\n",
        "transformer_model = build_transformer_classifier()\n",
        "_ = compile_and_train(transformer_model, \"Transformer encoder classifier\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36c6b9a8",
      "metadata": {
        "id": "36c6b9a8"
      },
      "source": [
        "## 8) Summary\n",
        "\n",
        "What you implemented here corresponds to key ideas in the chapter:\n",
        "\n",
        "- **RNN-based sentiment classifier:** Embedding → (Bi)GRU → Dense(sigmoid)\n",
        "- **Masking:** ignore padding tokens (either via `mask_zero=True` or explicit attention masks)\n",
        "- **Attention pooling:** scores → softmax weights → weighted sum for a single sequence representation\n",
        "- **Self-attention / Transformer blocks:** Multi-head attention + feed-forward + residual + layer norm + positional information\n",
        "\n",
        "If you include this notebook in GitHub, you can add a short README describing:\n",
        "- dataset used (IMDb)\n",
        "- model variants compared\n",
        "- any training results you observe when increasing epochs\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
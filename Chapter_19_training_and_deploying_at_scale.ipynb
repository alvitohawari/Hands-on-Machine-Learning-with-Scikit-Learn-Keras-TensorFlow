{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvitohawari/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-TensorFlow/blob/main/Chapter_19_training_and_deploying_at_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 19 – Training and Deploying TensorFlow Models at Scale (Summary)\n",
        "\n",
        "## Overview\n",
        "This chapter discusses how to **deploy, serve, scale, and optimize TensorFlow models in real-world production environments**.\n",
        "Training a high-performing model is only the first step—production systems require reliable deployment,\n",
        "version management, scalability, low latency, and efficient resource usage.\n",
        "\n",
        "The chapter covers:\n",
        "- Model serving using **TensorFlow Serving**\n",
        "- Model versioning and A/B testing\n",
        "- Deployment on **cloud platforms (Google Cloud AI Platform)**\n",
        "- Secure and scalable prediction services\n",
        "- Deployment to **mobile, embedded devices, and web browsers**\n",
        "- Model optimization techniques such as **quantization**\n",
        "\n",
        "---\n",
        "\n",
        "## Why Model Serving Is Necessary\n",
        "As systems grow, calling `model.predict()` directly inside applications becomes impractical.\n",
        "Instead, models are wrapped inside **dedicated prediction services**, which provide:\n",
        "- Centralized model access\n",
        "- Independent scaling\n",
        "- Easy model updates and rollbacks\n",
        "- Consistent predictions across systems\n",
        "- Support for **A/B experiments** and canary releases\n",
        "\n",
        "This decoupling improves reliability, maintainability, and scalability.\n",
        "\n",
        "---\n",
        "\n",
        "## SavedModel Format\n",
        "Before deployment, TensorFlow models must be exported to the **SavedModel** format.\n",
        "\n",
        "A SavedModel includes:\n",
        "- The computation graph\n",
        "- Model weights\n",
        "- Optional assets (e.g., vocabularies, class names)\n",
        "\n",
        "SavedModels:\n",
        "- Support **model versioning**\n",
        "- Are required by TensorFlow Serving\n",
        "- Can include preprocessing layers to avoid training–serving mismatches\n",
        "\n",
        "Each model version is stored in a separate directory, enabling smooth transitions between versions.\n",
        "\n",
        "---\n",
        "\n",
        "## TensorFlow Serving\n",
        "**TensorFlow Serving** is a high-performance, production-ready model server written in C++.\n",
        "\n",
        "Key features:\n",
        "- Serves multiple models and multiple versions simultaneously\n",
        "- Automatically loads the latest model version\n",
        "- Supports **graceful model updates**\n",
        "- Enables **automatic request batching** for higher throughput\n",
        "- Exposes models via **REST** and **gRPC** APIs\n",
        "\n",
        "TF Serving is commonly deployed using **Docker containers**, making installation and scaling straightforward.\n",
        "\n",
        "---\n",
        "\n",
        "## REST vs gRPC APIs\n",
        "TensorFlow Serving supports two main APIs:\n",
        "\n",
        "### REST API\n",
        "- Simple and widely supported\n",
        "- Uses JSON over HTTP\n",
        "- Easy to debug and integrate\n",
        "- Less efficient for large inputs due to text-based serialization\n",
        "\n",
        "### gRPC API\n",
        "- Binary protocol based on Protocol Buffers\n",
        "- Much more efficient and faster\n",
        "- Lower latency and bandwidth usage\n",
        "- Preferred for high-throughput production systems\n",
        "\n",
        "---\n",
        "\n",
        "## Model Versioning and Rollback\n",
        "TensorFlow Serving continuously monitors model directories.\n",
        "When a new version appears:\n",
        "- It loads the new version automatically\n",
        "- Handles pending requests gracefully\n",
        "- Unloads the old version once no longer needed\n",
        "\n",
        "Rolling back a model is as simple as removing the new version directory.\n",
        "This makes experimentation and recovery from failures safe and fast.\n",
        "\n",
        "---\n",
        "\n",
        "## Scaling with Load Balancing\n",
        "To handle high traffic:\n",
        "- Multiple TF Serving instances can be deployed\n",
        "- Requests are distributed using a **load balancer**\n",
        "- Container orchestration tools like **Kubernetes** simplify management\n",
        "\n",
        "Scaling ensures the system can handle high queries per second (QPS) reliably.\n",
        "\n",
        "---\n",
        "\n",
        "## Cloud Deployment with Google Cloud AI Platform\n",
        "The chapter demonstrates deploying models on **Google Cloud AI Platform**, which internally uses TensorFlow Serving.\n",
        "\n",
        "Benefits:\n",
        "- Automatic scaling based on traffic\n",
        "- Integrated monitoring and logging\n",
        "- Secure access via authentication\n",
        "- Pay-as-you-go pricing model\n",
        "\n",
        "Models are stored in **Google Cloud Storage (GCS)** and served through managed infrastructure.\n",
        "\n",
        "---\n",
        "\n",
        "## Authentication and Security\n",
        "Production services require secure access.\n",
        "\n",
        "Google Cloud uses:\n",
        "- **Service accounts** instead of user credentials\n",
        "- Token-based authentication\n",
        "- Fine-grained access control\n",
        "\n",
        "Client applications authenticate using service account keys, ensuring security and minimal permissions.\n",
        "\n",
        "---\n",
        "\n",
        "## Deploying to Mobile and Embedded Devices\n",
        "Large models are often unsuitable for mobile or embedded environments due to:\n",
        "- Limited memory\n",
        "- Limited computation power\n",
        "- Battery and latency constraints\n",
        "\n",
        "**TensorFlow Lite (TFLite)** addresses these challenges by:\n",
        "- Converting models to lightweight FlatBuffer format\n",
        "- Removing unnecessary operations\n",
        "- Optimizing computation graphs\n",
        "\n",
        "---\n",
        "\n",
        "## Model Optimization with Quantization\n",
        "Quantization reduces model size and improves efficiency by using lower-precision numbers.\n",
        "\n",
        "Techniques include:\n",
        "- **Post-training quantization** (e.g., 8-bit integers)\n",
        "- **Full integer quantization** (weights and activations)\n",
        "- **Quantization-aware training** to reduce accuracy loss\n",
        "\n",
        "These methods significantly reduce storage size, power consumption, and inference latency.\n",
        "\n",
        "---\n",
        "\n",
        "## TensorFlow in the Browser\n",
        "Models can also run directly in web browsers using **TensorFlow.js**.\n",
        "\n",
        "Advantages:\n",
        "- No server required for inference\n",
        "- Low latency\n",
        "- Improved privacy (data stays on the client)\n",
        "- Works offline or with poor connectivity\n",
        "\n",
        "Models are converted to a web-friendly format and executed using JavaScript and WebGL.\n",
        "\n"
      ],
      "metadata": {
        "id": "VbiL_baYnaVd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ksu0Ui652h"
      },
      "source": [
        "# Setup\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "503DvkwJ652h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcabdf3f-bad2-4b5c-f3d6-717821f42ed0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2943  100  2943    0     0  12283      0 --:--:-- --:--:-- --:--:-- 12365\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Hit:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,917 B in 1s (2,765 B/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttp://storage.googleapis.com/tensorflow-serving-apt/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tensorflow-model-server is already the newest version (2.19.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "No GPU was detected. CNNs can be very slow without a GPU.\n",
            "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    !echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" > /etc/apt/sources.list.d/tensorflow-serving.list\n",
        "    !curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "    !apt update && apt-get install -y tensorflow-model-server\n",
        "    %pip install -q -U tensorflow-serving-api\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deploy\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZpT3VXs652j"
      },
      "source": [
        "# Deploying TensorFlow models to TensorFlow Serving (TFS)\n",
        "We will use the REST API or the gRPC API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4q2ZdQR652j"
      },
      "source": [
        "## Save/Load a `SavedModel`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iGphitkD652j"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_new = X_test[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nDwvakoC652j",
        "outputId": "d84832b6-9e21-4474-9b67-1a54a0843466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.7269 - loss: 1.0513 - val_accuracy: 0.9044 - val_loss: 0.3649\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8982 - loss: 0.3667 - val_accuracy: 0.9188 - val_loss: 0.2954\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9148 - loss: 0.3059 - val_accuracy: 0.9302 - val_loss: 0.2623\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9233 - loss: 0.2721 - val_accuracy: 0.9352 - val_loss: 0.2395\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9305 - loss: 0.2479 - val_accuracy: 0.9384 - val_loss: 0.2219\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9360 - loss: 0.2288 - val_accuracy: 0.9416 - val_loss: 0.2078\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9405 - loss: 0.2130 - val_accuracy: 0.9472 - val_loss: 0.1959\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9435 - loss: 0.1996 - val_accuracy: 0.9500 - val_loss: 0.1856\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.1878 - val_accuracy: 0.9532 - val_loss: 0.1765\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9490 - loss: 0.1774 - val_accuracy: 0.9554 - val_loss: 0.1685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d70f1e8a450>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-GsKw_0h652k",
        "outputId": "8e4f3d6c-ec7c-40fe-b716-5ccb857041e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "np.round(model.predict(X_new), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3DgGUKi-652k",
        "outputId": "e1735d0f-16b8-4c23-c510-5ae89e7dd06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my_mnist_model/0001'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model_version = \"0001\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "khjYtBmG652k"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "if os.path.exists(model_name):\n",
        "    shutil.rmtree(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TkIplZva652k",
        "outputId": "147c921b-573d-49db-f685-5546abce70b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at 'my_mnist_model/0001'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor_8')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137924100800336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924100791312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924100795152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924100792080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "model.export(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_1N9_X62652k",
        "outputId": "5b168ec6-9cdf-469d-d270-4be41bcd395e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_mnist_model/\n",
            "    0001/\n",
            "        fingerprint.pb\n",
            "        saved_model.pb\n",
            "        variables/\n",
            "            variables.index\n",
            "            variables.data-00000-of-00001\n",
            "        assets/\n"
          ]
        }
      ],
      "source": [
        "for root, dirs, files in os.walk(model_name):\n",
        "    indent = '    ' * root.count(os.sep)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    for filename in files:\n",
        "        print('{}{}'.format(indent + '    ', filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VXPl5jcN652k",
        "outputId": "b6486fa9-b7d7-45ba-f6e9-d6686b0247f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-10 07:01:52.201873: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "The given SavedModel contains the following tag-sets:\n",
            "'serve'\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli show --dir {model_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Mj8Ulh-a652l",
        "outputId": "19a9dbf7-5b77-4e21-e514-88c9e52e4196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-10 07:02:02.959479: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
            "SignatureDef key: \"__saved_model_init_op\"\n",
            "SignatureDef key: \"serve\"\n",
            "SignatureDef key: \"serving_default\"\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "THXiAk_X652l",
        "outputId": "0201257d-73ce-4b1f-a518-35a7bea3d69d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-10 07:02:12.081381: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['keras_tensor_8'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 28, 28, 1)\n",
            "      name: serving_default_keras_tensor_8:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['output_0'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 10)\n",
            "      name: StatefulPartitionedCall_1:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli show --dir {model_path} --tag_set serve \\\n",
        "                      --signature_def serving_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fFsyYja5652l",
        "outputId": "cb3757be-5f1c-4ae5-fc67-e59cb4a45ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-10 07:02:20.836621: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serve']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['keras_tensor_8'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serve_keras_tensor_8:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['output_0'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['keras_tensor_8'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 28, 28, 1)\n",
            "        name: serving_default_keras_tensor_8:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['output_0'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 10)\n",
            "        name: StatefulPartitionedCall_1:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "The MetaGraph with tag set ['serve'] contains the following ops: {'Const', 'Placeholder', 'Identity', 'ReadVariableOp', 'MatMul', 'Softmax', 'StaticRegexFullMatch', 'StringJoin', 'Reshape', 'ShardedFilename', 'AssignVariableOp', 'BiasAdd', 'SaveV2', 'MergeV2Checkpoints', 'Relu', 'Select', 'Pack', 'VarHandleOp', 'DisableCopyOnRead', 'StatefulPartitionedCall', 'RestoreV2', 'NoOp', 'VarIsInitializedOp'}\n",
            "\n",
            "Concrete Functions:\n",
            "  Function Name: 'serve'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          keras_tensor_8: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor_8')\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli show --dir {model_path} --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t_SSvFC652l"
      },
      "source": [
        "Let's write the new instances to a `npy` file so we can pass them easily to our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Nxxn1Vgs652l"
      },
      "outputs": [],
      "source": [
        "np.save(\"my_mnist_tests.npy\", X_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "s6GiTgv2652l",
        "outputId": "95bde74e-beb0-4663-8eab-b3ee333f79f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keras_tensor_8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "input_name = 'keras_tensor_8'\n",
        "input_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLQ1QIoP652l"
      },
      "source": [
        "And now let's use `saved_model_cli` to make predictions for the instances we just saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2R3R7IWw652l",
        "outputId": "11cc1590-06db-4c31-9bb6-e664e1b18170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-10 07:04:50.587730: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/tools/saved_model_cli.py:716: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.saved_model.load` instead.\n",
            "W0110 07:04:50.603549 138241714081792 deprecation.py:50] From /usr/local/lib/python3.12/dist-packages/tensorflow/python/tools/saved_model_cli.py:716: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.saved_model.load` instead.\n",
            "INFO:tensorflow:Restoring parameters from my_mnist_model/0001/variables/variables\n",
            "I0110 07:04:50.628452 138241714081792 saver.py:1417] Restoring parameters from my_mnist_model/0001/variables/variables\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1768028690.633095    8285 mlir_graph_optimization_pass.cc:437] MLIR V1 optimization pass is not enabled\n",
            "Result for output key output_0:\n",
            "[[3.3691311e-05 4.6527626e-07 5.0736702e-04 3.6078931e-03 2.8341301e-06\n",
            "  2.1192979e-04 3.4250455e-08 9.9535841e-01 4.0768628e-05 2.3662121e-04]\n",
            " [1.4891347e-04 2.9136705e-05 9.9279636e-01 3.9052186e-03 1.6616211e-09\n",
            "  1.1385810e-03 1.8202194e-03 1.1247676e-10 1.6150183e-04 1.3086294e-09]\n",
            " [1.4460822e-05 9.8868364e-01 4.0191868e-03 1.1572685e-03 5.5852073e-04\n",
            "  6.1018759e-04 7.2173355e-04 2.3302133e-03 1.6763437e-03 2.2848442e-04]]\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli run --dir {model_path} --tag_set serve \\\n",
        "                     --signature_def serving_default    \\\n",
        "                     --inputs {input_name}=my_mnist_tests.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uRrovXll652l",
        "outputId": "1ef74a30-706a-4722-b79b-5a4ed9235389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.97, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "np.round([[1.1347984e-04, 1.5187356e-07, 9.7032893e-04, 2.7640699e-03, 3.7826971e-06,\n",
        "           7.6876910e-05, 3.9140293e-08, 9.9559116e-01, 5.3502394e-05, 4.2665208e-04],\n",
        "          [8.2443521e-04, 3.5493889e-05, 9.8826385e-01, 7.0466995e-03, 1.2957400e-07,\n",
        "           2.3389691e-04, 2.5639210e-03, 9.5886099e-10, 1.0314899e-03, 8.7952529e-08],\n",
        "          [4.4693781e-05, 9.7028232e-01, 9.0526715e-03, 2.2641101e-03, 4.8766597e-04,\n",
        "           2.8800720e-03, 2.2714981e-03, 8.3753867e-03, 4.0439744e-03, 2.9759688e-04]], 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTe7ux0d652l"
      },
      "source": [
        "## TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hRIFF4l652l"
      },
      "source": [
        "Install [Docker](https://docs.docker.com/install/) if you don't have it already. Then run:\n",
        "\n",
        "```bash\n",
        "docker pull tensorflow/serving\n",
        "\n",
        "export ML_PATH=$HOME/ml # or wherever this project is\n",
        "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
        "   -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
        "   -e MODEL_NAME=my_mnist_model \\\n",
        "   tensorflow/serving\n",
        "```\n",
        "Once you are finished using it, press Ctrl-C to shut down the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnSf2y-R652l"
      },
      "source": [
        "Alternatively, if `tensorflow_model_server` is installed (e.g., if you are running this notebook in Colab), then the following 3 cells will start the server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pJBRZuKF652l"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MODEL_DIR\"] = os.path.split(os.path.abspath(model_path))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XP61twsn652l"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "nohup tensorflow_model_server \\\n",
        "     --rest_api_port=8501 \\\n",
        "     --model_name=my_mnist_model \\\n",
        "     --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nP2tG5OI652m"
      },
      "outputs": [],
      "source": [
        "!tail server.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "j5wFXYHx652m"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "input_data_json = json.dumps({\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": X_new.tolist(),\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4dTTrrNi652m",
        "outputId": "0bd8d8e5-735f-44dd-deb6-577b6aba5560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.32941177487373...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "repr(input_data_json)[:1500] + \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtGIOV_f652m"
      },
      "source": [
        "Now let's use TensorFlow Serving's REST API to make predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ofZ5gAYC652m"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status() # raise an exception in case of error\n",
        "response = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VJ2I1wRz652m",
        "outputId": "35fa4269-4867-4819-e411-3b69080d6dca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['predictions'])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "response.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EuW_7qyM652m",
        "outputId": "9a985df9-f91d-4eb0-fa6f-d8821f2a4162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeHKQQP652m"
      },
      "source": [
        "### Using the gRPC API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "glyl9BRU652m"
      },
      "outputs": [],
      "source": [
        "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
        "\n",
        "request = PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = \"serving_default\"\n",
        "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4cY5EhqF652m"
      },
      "outputs": [],
      "source": [
        "import grpc\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "\n",
        "channel = grpc.insecure_channel('localhost:8500')\n",
        "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "response = predict_service.Predict(request, timeout=10.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "9b6jaW38652r",
        "outputId": "c3e4f3ab-79f5-4e1b-8dfd-2260bf2b0819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "outputs {\n",
              "  key: \"output_0\"\n",
              "  value {\n",
              "    dtype: DT_FLOAT\n",
              "    tensor_shape {\n",
              "      dim {\n",
              "        size: 3\n",
              "      }\n",
              "      dim {\n",
              "        size: 10\n",
              "      }\n",
              "    }\n",
              "    float_val: 3.36913108e-05\n",
              "    float_val: 4.65276258e-07\n",
              "    float_val: 0.000507367\n",
              "    float_val: 0.00360789313\n",
              "    float_val: 2.83413e-06\n",
              "    float_val: 0.000211929786\n",
              "    float_val: 3.42504549e-08\n",
              "    float_val: 0.995358407\n",
              "    float_val: 4.07686275e-05\n",
              "    float_val: 0.000236621214\n",
              "    float_val: 0.000148913474\n",
              "    float_val: 2.91367051e-05\n",
              "    float_val: 0.992796361\n",
              "    float_val: 0.00390521856\n",
              "    float_val: 1.66162106e-09\n",
              "    float_val: 0.00113858096\n",
              "    float_val: 0.00182021945\n",
              "    float_val: 1.12476757e-10\n",
              "    float_val: 0.000161501826\n",
              "    float_val: 1.30862943e-09\n",
              "    float_val: 1.44608221e-05\n",
              "    float_val: 0.988683641\n",
              "    float_val: 0.00401918683\n",
              "    float_val: 0.00115726853\n",
              "    float_val: 0.000558520725\n",
              "    float_val: 0.000610187592\n",
              "    float_val: 0.000721733551\n",
              "    float_val: 0.00233021332\n",
              "    float_val: 0.00167634373\n",
              "    float_val: 0.000228484423\n",
              "  }\n",
              "}\n",
              "model_spec {\n",
              "  name: \"my_mnist_model\"\n",
              "  version {\n",
              "    value: 1\n",
              "  }\n",
              "  signature_name: \"serving_default\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbMuwqzI652r"
      },
      "source": [
        "Convert the response to a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "scrolled": true,
        "id": "2Je2R5ds652r",
        "outputId": "f9cc660a-b530-45eb-fb35-d5add8882c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "output_name = \"output_0\"\n",
        "outputs_proto = response.outputs[output_name]\n",
        "y_proba = tf.make_ndarray(outputs_proto)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8coaTIgD652r"
      },
      "source": [
        "Or to a NumPy array if your client does not include the TensorFlow library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IU43MhnK652r",
        "outputId": "70ca3086-ad56-4173-c17c-08713c20d9ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "output_name = \"output_0\"\n",
        "outputs_proto = response.outputs[output_name]\n",
        "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
        "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx5pUHtn652r"
      },
      "source": [
        "## Deploying a new model version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "scrolled": true,
        "id": "ly355J4z652r",
        "outputId": "c141e426-04c3-47e2-f575-6b8ff8a2b4f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6617 - loss: 1.1876 - val_accuracy: 0.9046 - val_loss: 0.3526\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9004 - loss: 0.3504 - val_accuracy: 0.9254 - val_loss: 0.2773\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9169 - loss: 0.2880 - val_accuracy: 0.9304 - val_loss: 0.2415\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9265 - loss: 0.2521 - val_accuracy: 0.9370 - val_loss: 0.2166\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9348 - loss: 0.2243 - val_accuracy: 0.9434 - val_loss: 0.1972\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9413 - loss: 0.2020 - val_accuracy: 0.9474 - val_loss: 0.1817\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9464 - loss: 0.1841 - val_accuracy: 0.9504 - val_loss: 0.1692\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1693 - val_accuracy: 0.9538 - val_loss: 0.1590\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9542 - loss: 0.1571 - val_accuracy: 0.9566 - val_loss: 0.1508\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9574 - loss: 0.1465 - val_accuracy: 0.9594 - val_loss: 0.1436\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.Input(shape=[28, 28, 1]),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(50, activation=\"relu\"),\n",
        "    keras.layers.Dense(50, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PE8ian-a652s",
        "outputId": "d0fe9880-07e7-4083-f5e7-f0d29dedd9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my_mnist_model/0002'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "model_version = \"0002\"\n",
        "model_name = \"my_mnist_model\"\n",
        "model_path = os.path.join(model_name, model_version)\n",
        "model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PcuxngvB652s",
        "outputId": "90aea419-4bc3-40d3-b8df-be2ec3ea8331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at 'my_mnist_model/0002'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor_12')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137924661670928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924661671696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924661671504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924661668816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924661672272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137924661671888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "model.export(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "FgYF7rNs652s",
        "outputId": "4041f206-3876-4dad-92a5-92d34157bdff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_mnist_model/\n",
            "    0001/\n",
            "        fingerprint.pb\n",
            "        saved_model.pb\n",
            "        variables/\n",
            "            variables.index\n",
            "            variables.data-00000-of-00001\n",
            "        assets/\n"
          ]
        }
      ],
      "source": [
        "for root, dirs, files in os.walk(model_name):\n",
        "    indent = '    ' * root.count(os.sep)\n",
        "    print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "    for filename in files:\n",
        "        print('{}{}'.format(indent + '    ', filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-TBF_CH652s"
      },
      "source": [
        "**Warning**: You may need to wait a minute before the new model is loaded by TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "7OEoSGIo652s"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
        "\n",
        "response = requests.post(SERVER_URL, data=input_data_json)\n",
        "response.raise_for_status()\n",
        "response = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ghmYjPe0652s",
        "outputId": "1c62a68a-eeff-495f-ae0a-2dab10c58005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['predictions'])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "response.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6vCtEQrJ652s",
        "outputId": "335d4609-92d1-4070-d2de-1e47da036def",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.  ],\n",
              "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
              "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "y_proba = np.array(response[\"predictions\"])\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAwulaFu652s"
      },
      "source": [
        "# Deploy the model to Google Cloud AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOUgct0652s"
      },
      "source": [
        "Follow the instructions in the book to deploy the model to Google Cloud AI Platform, download the service account's private key and save it to the `my_service_account_private_key.json` in the project directory. Also, update the `project_id`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "lhqGBRpK652s"
      },
      "outputs": [],
      "source": [
        "project_id = \"onyx-smoke-242003\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "jLJfJwYp652s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "b39eec93-d873-427d-8e8b-7ffef303415c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "DefaultCredentialsError",
          "evalue": "File my_service_account_private_key.json was not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3469950134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"projects/{}/models/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"/versions/v0001/\"\u001b[0m \u001b[0;31m# if you want to run a specific version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mml_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/discovery.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(serviceName, version, http, discoveryServiceUrl, developerKey, model, requestBuilder, credentials, cache_discovery, cache, client_options, adc_cert_path, adc_key_path, num_retries, static_discovery, always_use_jwt_access)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mstatic_discovery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatic_discovery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             )\n\u001b[0;32m--> 315\u001b[0;31m             service = build_from_document(\n\u001b[0m\u001b[1;32m    316\u001b[0m                 \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscovery_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/discovery.py\u001b[0m in \u001b[0;36mbuild_from_document\u001b[0;34m(service, base, future, http, developerKey, model, requestBuilder, credentials, client_options, adc_cert_path, adc_key_path, always_use_jwt_access)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# default credentials.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                 credentials = _auth.default_credentials(\n\u001b[0m\u001b[1;32m    617\u001b[0m                     \u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/_auth.py\u001b[0m in \u001b[0;36mdefault_credentials\u001b[0;34m(scopes, quota_project_id)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;34m\"\"\"Returns Application Default Credentials.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mHAS_GOOGLE_AUTH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         credentials, _ = google.auth.default(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchecker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             credentials = with_scopes_if_required(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;31m# safely set on the returned credentials since requires_scopes will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;31m# guard against setting scopes on user credentials.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_get_explicit_environ_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_get_gcloud_sdk_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0m_get_gae_credentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36m_get_explicit_environ_credentials\u001b[0;34m(quota_project_id)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             credentials, project_id = load_credentials_from_file(\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvironment_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCREDENTIALS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mload_credentials_from_file\u001b[0;34m(filename, scopes, default_scopes, quota_project_id, request)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         raise exceptions.DefaultCredentialsError(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;34m\"File {} was not found.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         )\n",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m: File my_service_account_private_key.json was not found."
          ]
        }
      ],
      "source": [
        "import googleapiclient.discovery\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_private_key.json\"\n",
        "model_id = \"my_mnist_model\"\n",
        "model_path = \"projects/{}/models/{}\".format(project_id, model_id)\n",
        "model_path += \"/versions/v0001/\" # if you want to run a specific version\n",
        "ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjmsqhhd652s"
      },
      "outputs": [],
      "source": [
        "def predict(X):\n",
        "    input_data_json = {\"signature_name\": \"serving_default\",\n",
        "                       \"instances\": X.tolist()}\n",
        "    request = ml_resource.predict(name=model_path, body=input_data_json)\n",
        "    response = request.execute()\n",
        "    if \"error\" in response:\n",
        "        raise RuntimeError(response[\"error\"])\n",
        "    return np.array([pred[output_name] for pred in response[\"predictions\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbysLnc9652s"
      },
      "outputs": [],
      "source": [
        "Y_probas = predict(X_new)\n",
        "np.round(Y_probas, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYUrcxrn652s"
      },
      "source": [
        "# Using GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZamQ3ys652t"
      },
      "source": [
        "**Note**: `tf.test.is_gpu_available()` is deprecated. Instead, please use `tf.config.list_physical_devices('GPU')`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Nqaz2B3s652t",
        "outputId": "f8f30af0-2ceb-43c6-9109-68da01de87ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "#tf.test.is_gpu_available() # deprecated\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "lIU4nSOo652t",
        "outputId": "1dd31700-272b-4ddd-9523-5bce8a5a77a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jkblX1Oe652t",
        "outputId": "59f85807-13d8-4224-9f48-18bd263c8d84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "tf.test.is_built_with_cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gp-RZlu_652t",
        "outputId": "7e928a9e-b9ff-493e-e444-bad35c35ac7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 2443272670311274920\n",
              " xla_global_id: -1]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "from tensorflow.python.client.device_lib import list_local_devices\n",
        "\n",
        "devices = list_local_devices()\n",
        "devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwCC0QHA652t"
      },
      "source": [
        "# Distributed Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "7pVAL8-Y652t"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "4aA0zCyT652t"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    return keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
        "                            padding=\"same\", input_shape=[28, 28, 1]),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"),\n",
        "        keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
        "                            padding=\"same\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=2),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(units=64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(units=10, activation='softmax'),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "0DEYHaHd652t",
        "outputId": "71d783e9-da89-4f86-f6b8-3161a9893e07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 894ms/step - accuracy: 0.3701 - loss: 1.8643 - val_accuracy: 0.9030 - val_loss: 0.3484\n",
            "Epoch 2/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 895ms/step - accuracy: 0.8364 - loss: 0.5218 - val_accuracy: 0.9448 - val_loss: 0.1879\n",
            "Epoch 3/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 897ms/step - accuracy: 0.9016 - loss: 0.3353 - val_accuracy: 0.9638 - val_loss: 0.1330\n",
            "Epoch 4/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 888ms/step - accuracy: 0.9222 - loss: 0.2616 - val_accuracy: 0.9686 - val_loss: 0.1040\n",
            "Epoch 5/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 908ms/step - accuracy: 0.9394 - loss: 0.2096 - val_accuracy: 0.9726 - val_loss: 0.0916\n",
            "Epoch 6/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 894ms/step - accuracy: 0.9463 - loss: 0.1855 - val_accuracy: 0.9754 - val_loss: 0.0806\n",
            "Epoch 7/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 898ms/step - accuracy: 0.9518 - loss: 0.1675 - val_accuracy: 0.9768 - val_loss: 0.0752\n",
            "Epoch 8/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 907ms/step - accuracy: 0.9570 - loss: 0.1494 - val_accuracy: 0.9774 - val_loss: 0.0700\n",
            "Epoch 9/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 888ms/step - accuracy: 0.9604 - loss: 0.1354 - val_accuracy: 0.9808 - val_loss: 0.0641\n",
            "Epoch 10/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 903ms/step - accuracy: 0.9633 - loss: 0.1243 - val_accuracy: 0.9832 - val_loss: 0.0596\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d711695baa0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "batch_size = 100\n",
        "model = create_model()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "dRtXZlLb652t"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# Change the default all-reduce algorithm:\n",
        "#distribution = tf.distribute.MirroredStrategy(\n",
        "#    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "\n",
        "# Specify the list of GPUs to use:\n",
        "#distribution = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
        "\n",
        "# Use the central storage strategy instead:\n",
        "#distribution = tf.distribute.experimental.CentralStorageStrategy()\n",
        "\n",
        "#if IS_COLAB and \"COLAB_TPU_ADDR\" in os.environ:\n",
        "#  tpu_address = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
        "#else:\n",
        "#  tpu_address = \"\"\n",
        "#resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "#tf.config.experimental_connect_to_cluster(resolver)\n",
        "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#distribution = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "                  metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GFA8eLlC652t",
        "outputId": "8b9d5a52-217c-4eb9-fd55-a883b7df5cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 894ms/step - accuracy: 0.3542 - loss: 1.9191 - val_accuracy: 0.9020 - val_loss: 0.3596\n",
            "Epoch 2/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 910ms/step - accuracy: 0.8286 - loss: 0.5573 - val_accuracy: 0.9446 - val_loss: 0.2039\n",
            "Epoch 3/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 909ms/step - accuracy: 0.8960 - loss: 0.3527 - val_accuracy: 0.9600 - val_loss: 0.1459\n",
            "Epoch 4/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 900ms/step - accuracy: 0.9202 - loss: 0.2661 - val_accuracy: 0.9686 - val_loss: 0.1128\n",
            "Epoch 5/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 912ms/step - accuracy: 0.9352 - loss: 0.2202 - val_accuracy: 0.9740 - val_loss: 0.0897\n",
            "Epoch 6/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 897ms/step - accuracy: 0.9462 - loss: 0.1854 - val_accuracy: 0.9768 - val_loss: 0.0812\n",
            "Epoch 7/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 885ms/step - accuracy: 0.9529 - loss: 0.1630 - val_accuracy: 0.9776 - val_loss: 0.0747\n",
            "Epoch 8/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 896ms/step - accuracy: 0.9564 - loss: 0.1506 - val_accuracy: 0.9806 - val_loss: 0.0688\n",
            "Epoch 9/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 897ms/step - accuracy: 0.9598 - loss: 0.1374 - val_accuracy: 0.9810 - val_loss: 0.0644\n",
            "Epoch 10/10\n",
            "\u001b[1m550/550\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 889ms/step - accuracy: 0.9622 - loss: 0.1292 - val_accuracy: 0.9792 - val_loss: 0.0648\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d70f2289010>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "batch_size = 100 # must be divisible by the number of workers\n",
        "model.fit(X_train, y_train, epochs=10,\n",
        "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "RB8FhlBg652t",
        "outputId": "4404c3b9-4138-48b8-da42-614e7c6d0262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5360947e-10, 6.7660409e-08, 1.6691367e-06, 1.5719796e-06,\n",
              "        1.4532838e-09, 5.0937308e-09, 6.3870675e-13, 9.9997663e-01,\n",
              "        1.3030483e-09, 2.0081137e-05],\n",
              "       [3.4333658e-07, 3.8386035e-05, 9.9995732e-01, 3.9597112e-06,\n",
              "        2.2954702e-11, 2.4577980e-09, 3.9073328e-08, 1.4761659e-10,\n",
              "        4.8106042e-08, 5.0022001e-15],\n",
              "       [1.1794024e-06, 9.9952281e-01, 2.6604683e-05, 1.8608032e-06,\n",
              "        1.5183409e-04, 1.1377758e-06, 9.9319812e-05, 1.7172418e-04,\n",
              "        2.0697747e-05, 2.7802221e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuPUG2Vx652t"
      },
      "source": [
        "Custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-Df8u_Fq652t",
        "outputId": "76807b99-95f7-44ba-9554-f4124c7f9a7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-431697354.py:36: DistributedIteratorV1.initialize (from tensorflow.python.distribute.v1.input_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the iterator's `initializer` property instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py:460: StrategyBase.experimental_run (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use run() instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Loss: 0.428\n",
            "Epoch 2/10\n",
            "Loss: 0.333\n",
            "Epoch 3/10\n",
            "Loss: 0.305\n",
            "Epoch 4/10\n",
            "Loss: 0.290\n",
            "Epoch 5/10\n",
            "Loss: 0.286\n",
            "Epoch 6/10\n",
            "Loss: 0.282\n",
            "Epoch 7/10\n",
            "Loss: 0.282\n",
            "Epoch 8/10\n",
            "Loss: 0.282\n",
            "Epoch 9/10\n",
            "Loss: 0.283\n",
            "Epoch 10/10\n",
            "Loss: 0.281\n"
          ]
        }
      ],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "K = keras.backend\n",
        "\n",
        "distribution = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with distribution.scope():\n",
        "    model = create_model()\n",
        "    optimizer = keras.optimizers.SGD()\n",
        "\n",
        "with distribution.scope():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat().batch(batch_size)\n",
        "    input_iterator = distribution.make_dataset_iterator(dataset)\n",
        "\n",
        "@tf.function\n",
        "def train_step():\n",
        "    def step_fn(inputs):\n",
        "        X, y = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            Y_proba = model(X)\n",
        "            loss = K.sum(keras.losses.sparse_categorical_crossentropy(y, Y_proba)) / batch_size\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "    per_replica_losses = distribution.experimental_run(step_fn, input_iterator)\n",
        "    mean_loss = distribution.reduce(tf.distribute.ReduceOp.SUM,\n",
        "                                    per_replica_losses, axis=None)\n",
        "    return mean_loss\n",
        "\n",
        "n_epochs = 10\n",
        "with distribution.scope():\n",
        "    input_iterator.initialize()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
        "        for iteration in range(len(X_train) // batch_size):\n",
        "            print(\"\\rLoss: {:.3f}\".format(train_step().numpy()), end=\"\")\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}